\section{Reptile: a Scalable Meta Learning Algorithm by Alex Nichol and John Schulman, 2018}

\subsection{Motivation}

Non-gradient based approaches (e.g. using LSTM to predict weights) sometimes suffer from lack of generalization (overfitting).
While MAML achieves better generalization given it relies on SGD, it requires differentiation within the optimization process,
which is costly if we take a large number of gradient steps at test time. 

\subsection{Algorithm}
Let $SGD(L, \phi, k)$ denote the function that takes $k$ steps of SGD on loss $L$ starting with parameters 
$\phi$ and returns the final parameter vector. \\\\

Initalize model parameters $\phi$
For $i = 1, 2, \dots$:
\begin{itemize}
    \item Sample task $\tau$ from task distribution $D_{\tau}$, corresponding to loss $L_{\tau}$ on weight vectors $W$
    \item Compute $W = SGD(L_{\tau}, \phi, k)$
    \item $\phi = \phi + \epsilon (W - \phi)$
\end{itemize}

There is also a batched version of Reptile that works as follows: \\\\

Initialize $\phi$
For $i = 1, 2, \dots$
\begin{itemize}
    \item Sample $n$ tasks $\tau_1, \tau_2, \dots, \tau_n$ from $D_{\tau}$
    \item For $j = 1, 2, \dots, n$
    \begin{itemize}
        \item $W_j = SGD(L_j, \tau_j, k)$
    \end{itemize}
    \item $\phi = \phi + \epsilon \frac{1}{k}(\sum_{j = 1}^{n} W_j - \phi)$
\end{itemize}

Note that when $k = 1$ then Reptile is the same as updating $\phi$ by $\Delta_{\phi} \mathbb{E}_{\tau}(L_{\tau}(f_\phi))$.
However when $k > 1$ then the Reptile update is different than taking the derivative of the expected loss.

\subsection{Experiments}

\subsubsection{Sign Wave Regression:}

The task is to predict $y$ value given $x$ for a sign wave $\tau_i$ with certain amplitude and phase.
Where $L_{\tau} = \integral_{-5}^{5} dx \mid f(x) - f_{\tau}(x) \mid^2$, which is evaluated using 50 equally spaced points.
Training on the expected loss $\mathbb{E}_\tau[L_\tau]$ would not work because this loss is minimized by $f(x) = 0, \forall x$.
Therefore, it is promising that after training reptile converges instead to a sign wave that is close to the function $f_{\tau}$.

\subsection{Why it works}
