\section{One-Shot Visual Imitation Learning via Meta-Learning by Chelsea Finn et al. 2017}

\subsection{Summary:}
\textbf{Core Question:} How can we leverage information from previous skills to quickly learning
new behaviors?

Work forcuses on learning to imiation learn from one demonstration where only video of
the demonstration is available.

\subsection{Model:}

The policy we are trying to learn: $\pi: o \rightarrow \hat a$

Each imitation task $T_i = \{ \tau = \{ o_1, a_1, \dots, o_T, a_T \} \sim \pi^{*}_{i}, L(a_{1:T}, \hat a_{1:T}), T \}$
data $\tau$ is generated by an expert policy $\pi^{*}_{i}$ and loss function $L$.

MAML is extended to imitation learning as follows:

Let the demonstration trajectory (images and motor torques) be $\tau := \{o_1, a_1, \dots, o_T, a_T \}$.

They use a MSE loss function on the policy parameters $\phi$:
$$L_{T_i}(f_\phi) = \sum_{\tau^{(j) \sim T_i}} \sum_t \norm{f_{\phi}(o_t^{(j)}) - a_t^{(j)}}_2^2$$

\subsubsection{Two head architecture:}

The architecture uses two heads:
\begin{enumerate}
    \item pre-update head whose parameters are not used in post-update policy. (think this is $\theta$)
    \item post-update head which is not updated using the demonstration (think this is $\theta'$)
\end{enumerate}

\subsection{Training:}

\textbf{Meta-training loop:}
\begin{enumerate}
    \item Sample a batch of tasks and two demonstrations per task
    \item Using one of the demonstrations $\tau^{(1)} \sim T_i$ compute $\theta^{'}_{i}$ for each task $T_i$
    \item Using the second demonstration $\tau^{(2)} \sim T_i$ compute the gradient of the meta-objective
    \item Update $\theta$ using the gradient of the meta objective
          $\Delta_\theta \sum_{T_i \sim p(T)} L_{T_i}(f_{\theta^{'}_{i}})$ with the MSE loss given above
\end{enumerate}

\subsection{Experiments:}

\begin{enumerate}
    \item Simulated reaching
    \item Simulated pushing
    \item Real-World Placing
\end{enumerate}

\subsection{My Conclusions: (concerns, follow up, etc)}

Follow up:

\begin{itemize}
    \item How could you adversarially attack MAML? Are there any good opertunities here?
    \item Could you somehow use MAML objective as a fitness function in evolutionary algos?
\end{itemize}

